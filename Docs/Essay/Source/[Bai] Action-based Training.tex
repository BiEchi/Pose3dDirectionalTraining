\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[section]{placeins}
\usepackage{amssymb}
\usepackage{float}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}

\graphicspath{ {./graphs/} }

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[colorlinks=true, citecolor=blue]{hyperref}


\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{An Pose Estimation Training Method For \textit{VideoPose3D} with Action Recognition}

\author{Hao Bai\\
ZJU-UIUC Institute\\
Haining, Jiaxing\\
{\tt\small https://jackgetup.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%\textbf{Prof.} Gaoang Wang\\
%ZJU-UIUC Institute\\
%Haining, Jiaxing\\
%{\tt\small gaoangwang@intl.zju.edu.cn}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
	Action recognition and pose estimation from video are closely related tasks for understanding
	human motions, but the work to utilize both is rare in literature. In this essay I show an 
	action-recognition-based training method for \textit{VideoPose3D}, which contributes to the 
	precision of velocity error and speed of training. The model is fed with videos which have exactly 
	the same type of action in the estimation period, and it only requires a small amount of data. 
	Evidence has shown that under a limited amount of epochs and data, our approach outperforms 
	the original research. The code of this project, is available under the open-source MIT License at
	\url{https://github.com/BiEchi/Pose3dDirectionalTraining}. 
   

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Vision-based human motion analysis attempts to understand the movements of the human body 
using computer vision and machine learning techniques. Technically speaking, the main goal
for pose estimation and motion analysis is to rebuild the movements of the locations for 
human body joints inside videos. Qualitative speaking, lower estimation error means better 
performance.


The effect between pose estimation and action recognition is mutual. Pose estimation
and action recognition can utilize the same dataset \cite{jhuang2013towards, andriluka2018posetrack}
Pose estimation can be used for action recognition, with the conclusion that a lower
estimation error leads to a more accurate classification of action recognition \cite{yao2011does}. 
Turning around, action priors and also be used to ameliorate the precision of pose estimation 
\cite{yao2012coupled, iqbal2017pose, gall20102d}. There have also been work combining these two tasks 
together and perform better in each field \cite{7298734, luvizon20182d}.


In the field of 3D post estimation by deep learning approach, there are many successful creatures 
with various types of training methods, like Maximum-Margin Structured training by Li et al. 
\cite{li2015maximum}, Feedback Loop training by Oberweger et al.\cite{oberweger2015training}, and 
also Semi-Supervised training by Pavllo et al. \cite{pavllo20193d}, which we mainly concern with 
in our work.

\textit{VideoPose3D} has been very successful because of its high performance and SOTA precision.
This work utilizes the datasets \textbf{HumanEva} \cite{sigal2010humaneva} and \textbf{Human3.6M} 
\cite{ionescu2013human3}. \textbf{Human3.6M} has 11 subjects and 4 viewpoints in total, and 15 types of actions.
We've got our motivation that, \textit{VideoPose3D} takes a variety of (instead of a certain type of) actions 
as training data, but there have been lots of successful cases when using action recognition for pose estimations 
works well for improving its performance, which gives rise to our project.

%------------------------------------------------------------------------
\section{Related Works and Our Contributions}

As mentioned in the Introduction part, our work is based on the previous work \textit{VideoPose3D} 
\cite{pavllo20193d} and gets its motivation from successful cases to utilize action recognition for 
pose estimation \cite{yao2012coupled, iqbal2017pose, gall20102d}. In this part we illustrate how
these works model and how we can utilize them to perform a better work based on them.

%-------------------------------------------------------------------------
\subsection{VideoPose3D}

\textit{VideoPose3D} was the state-of-the-art approach which "utilizes a fully convolutional model based on dilated 
temporal convolutions over 2D keypoints" \cite{pavllo20193d}. In early researches the main solution was to use recurrent 
neural networks (RNN) \cite{lee2018propagating}, but the research \textit{VideoPose3D} utilized the convolutional 
neuron network (CNN) to gain an efficient result on the dataset \textit{Human3.6M}, with the inspiration that
many authors had mentioned CNN in temporal models.

Technically speaking, \textit{VideoPose3D} takes a series of images as the input of the training network to imitate the
functionality as RNN (Figure \ref{CNN_in_VideoPose3D}). This ideology has been widely accepted in the current academy and there have been lots of successful
cases utilizing this method, e.g. the Spatial-Temporal-CNN used for crowd counting in videos \cite{miao2019st}, and LSTM-CNN
used for face anti-spoofing \cite{xu2015learning}. From these researches we see the capability of CNN. However, CNN is not a natural temporal
network itself, which means it bears a trait of parallelism and lack of temporal memory \cite{shin2016deep}.

\begin{figure}[H]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  		\includegraphics[width=1.0\linewidth]{CNN_in_VideoPose3D.png}
	\end{center}
   	\caption{The temporal convilutional model from \textit{VideoPose3D} takes 2D keypoint sequences (bottom) as input and 
   			 generates 3D pose estimation as out (top).}
	\label{CNN_in_VideoPose3D}
\end{figure}


The overall model of \textit{VideoPose3D} is a semi-supervised learning method (Figure 1). 

\begin{figure}[H]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  		\includegraphics[width=0.9\linewidth]{original.png}
	\end{center}
   	\caption{The overall model of \textit{VideoPose3D}}
	\label{fig:long}
	\label{fig:onecol}
\end{figure}

%-------------------------------------------------------------------------
\subsection{Action Recognition}

The idea of homogenized learning was proposed firstly by Action Estimations \cite{wang2013action}. 
Hitherto, there have been a variety of researches focusing on the combination of action recognition and pose
estimation \cite{yao2012coupled}. The basic idea is to begin with 2D appearance-based action recognition
based on low-level appearance features. Outputs of the 2D action recognition are used as a prior distribution
for the particle-based optimization for 3D pose estimation. Finally, 3D pose-based action recognition is then 
performed based on pose features extracted from the estimated poses.

\begin{figure}[H]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  		\includegraphics[width=0.9\linewidth]{action_and_pose.png}
	\end{center}
   	\caption{The coupled action recognition and pose estimation framework forwarded by \cite{yao2012coupled}.}
	\label{fig:long}
	\label{fig:onecol}
\end{figure}


%-------------------------------------------------------------------------
\subsection{Homography Estimation}

The idea of using homonegeious actions in a dataset was also inspired from Homography Estimation
in some deep images \cite{detone2016deep}.



%------------------------------------------------------------------------
\section{Representation and Modeling}
 
The detailed experimental setup can also be explored in \textit{README.md} in the github repo. To reproduce
the results, you need to test for 15 times for each type of action, and it's preferable to test for multiple
times. It's more convenient to use \textit{bash} scripts to save time.


Our work focuses on using certain types of actions for training data. For example, say there are 4 actions for all
subjects, i.e. sitting, walking, discussing, and posing. The original \textit{VideoPose3D} model takes all 4 types
of data as training data (Figure 1, left), and they estimate results for all action in the end. Our model, instead, takes 
only one action as training data (Figure 1, right), and we estimate ONLY result for this certain action in the end. Absolutely, this
process will iterate through all actions if all of them need to be estimated.

\begin{figure*}
\centering
\includegraphics[width=8cm]{model.png}
\includegraphics[width=8cm]{new_model.png}
\caption{The comparison of the original model (left) and our model (right). The original model takes all types of actions for 
		 training, but we only take a certain type of data as training data, as an approach to utilize action recognition for pose estimation.}
\label{fig:long}
\end{figure*}

Compared to the original model we utilized a different data-processing approach. The original approach
mentioned in \textit{VideoPose3D} uses all actions in training data, and the estimating effect was good
overall. We utilize the method that we only take one type of action for training, and the total amount 
for training should be the same. Mathematically, take the action \textit{Sitting} for example, it can 
be expressed as*

\begin{equation}
	\sum_{i\in actions}{f(i)} = f(Sitting)\cdot t
\end{equation}

where $f$ stands for the frames of subject, and $t$ means the number of epochs of our action-based 
training. Note that the unit of $t$ is unit-epoch instead of epoch. The unit-epoch stands for 1 epoch
of one action. According to the dataset we utilize, \textit{Human3.6M}, there are 15 actions for each
subject. Thus, the relationship between epoch and unit-epoch can be expressed mathematically as below.

\begin{equation}
	t_0 = t_{unit}\cdot 15
\end{equation}


In this approach, we utilize only one time of action, which means we save more data; and we also produce
the results in the same time, the evidence can be illustrated by formula (1). After testing, we conclude 
that our work outperforms the original work by 11\%.

The principle of our work is shown below.

In our model we also changed the process of training in the original research. In the original research,
it takes 15 unit-epochs to run a single epoch, because there are 15 types of actions. In our work, we can
tweak the number of unit-epochs not necessarily to be a multiple of 15, which makes it more flexible to 
observe and iterate. Shown below is the algorithm for our iterative training when not so many epochs are 
available (we take 2 epochs for example).


\begin{algorithm}
  \caption{Original \textit{VideoPose3D} Model}
  \SetAlgoLined
  
  \KwData{Training data, testing data, ground-truth data, total training epoch = 2.}
  \KwResult{Error value.}
  
  \For{\textsc{epoch} in \textsc{2}}
  	{
 		\textsc{Err}=VideoPose3D(\textsc{Train, Test, GT})\;
  		print(\textsc{Err})\;
  		
  	}
  
 \Return{\textsc{Err}}\;
\end{algorithm}

\begin{algorithm}
  \caption{Our Action-based Approach}
  \SetAlgoLined
  
  \KwData{Training data, testing data, ground-truth data, total training epoch = 2, action type.}
  \KwResult{Error value.}
  
  \For{\textsc{epoch} in $2\cdot 15$}
  	{
 		\textsc{Err for Action}=\textbf{VideoPose3D}(\textsc{Train, Test, GT, Action})\;
  		\textbf{print}(\textsc{Err for Action})\;
  		
  	}
  
 \Return{\textsc{Err for Action}}\;
\end{algorithm}



%------------------------------------------------------------------------

\section{Experiments}

Shown below are the results of the experiment. Expect a minor error when testing on your own. Note that all
unit are millimeter.

\subsection{Small Number of Epochs}

In this part we estimate the errors with a small amount of time. The epoch of the original test was set to 
be 1 unit epoch, and the epoch of the action-based test was set to 15 epochs. In the table below (Table 1 \&
Table 2), we illustrate the pose estimation results with respect to different types of model with different
training arguments (i.e. the number of receptive field Frames \textbf{F}, the number of Unit Epochs \textbf{UE}),
15 different actions, and two different protocols (i.e. MPJPE and Velocity Error of MPJPE).

In the tables the model with the best performance is in bold, and we set the argument as the highest classification standard
for better comparison. 

In the figures below we visualize the data by different actions.


\begin{center}
\begin{table*}
\small
%\begin{tabular}{cl | p{0.4cm} p{0.4cm} p{0.3cm} p{0.5cm}p{0.6cm}p{0.5cm}p{0.3cm}p{0.4cm}p{0.1cm}p{0.4cm}p{0.7cm}p{0.4cm}p{0.8cm}p{0.4cm}p{0.8cm} p{0.3cm} } 
\setlength\tabcolsep{2.3pt}
\begin{tabular}{l|l|rrrrrrrrrrrrrrrr}
Arguments & Model & Dir. & Disc. & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WkD. & Walk & WkT. & \textbf{Avg} \\

\hline
1F, 15UE & VideoPose3D & 67.0 & \textbf{65.8} & 68.7 & 72.5 & \textbf{74.2} & \textbf{87.2} & \textbf{65.5} & \textbf{73.1} & 85.6 & 117 & 73.6 & \textbf{70.8} & 81.1 & 63.5 & 67.3 & 75.5 \\
1F, 15UE & Ours & \textbf{61.9} & 69.2 & \textbf{62.0} & \textbf{68.4} & 75.7 & 88.2 & 74.5 & 76.9 & \textbf{81.5} & \textbf{97.9} & \textbf{71.1} & 80.9 & \textbf{80.7} & \textbf{49.9} & \textbf{59.2} & \textbf{73.2}\\
\hline


27F, 15 UE & VideoPose3D & \textbf{54.5} & \textbf{61.4} & 56.3 & \textbf{58.6} & \textbf{61.3} & \textbf{68.6} & \textbf{57.6} & \textbf{60.6} & 70.5 & 84.7 & 60.5 & \textbf{59.1} & 68.2 & 51.8 & \textbf{53.1} & \textbf{61.8} \\
27F, 15 UE & Ours & 70.5 & 71.9 & \textbf{52.1} & 61.4 & 69.0 & 82.6 & 70.6 & 82.1 & \textbf{70.4} & \textbf{79.4} & \textbf{60.3} & 79.5 & \textbf{56.1} & \textbf{44.2} & 56.2 & 67.1\\
\hline

243F, 1200 UE & Pavlakos \cite{pavlakos2017coarse} & 67.4 & 71.9 & 66.7 & 69.1 & 72.0 & 77.0 & 65.0 & 68.3 & 83.7 & 96.5 & 71.7 & 65.8 & 74.9 & 59.1 & 63.2 & 71.9 \\
243F, 1200 UE & Luvizon \cite{luvizon20182d} & 49.2 & 51.6 & 47.6 & 50.5 & 51.8 & 60.3 & 48.5 & 51.7 & 61.5 & 70.9 & 53.7 & 48.9 & 57.9 & 44.4 & 48.9 & 53.2\\
243F, 1200 UE & VideoPose3D\\
243F, 1200 UE & Ours & & & 49.3\\
 
\hline 

\hline
\end{tabular}

\caption{Protocol 1, MPJPE Error}
\end{table*}
\end{center}


\begin{center}
\begin{table*}
\small
%\begin{tabular}{l | p{0.4cm} p{0.4cm} p{0.3cm} p{0.5cm}p{0.6cm}p{0.5cm}p{0.3cm}p{0.4cm}p{0.1cm}p{0.4cm}p{0.7cm}p{0.4cm}p{0.8cm}p{0.4cm}p{0.8cm} p{0.3cm} } 
\setlength\tabcolsep{2.3pt}
\begin{tabular}{l|l|rrrrrrrrrrrrrrrr} 
Arguments & Model & Dir. & Disc. & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WkD. & Walk & WkT. & \textbf{Avg} \\
\hline
1F, 15 UE & VideoPose3D & \textbf{10.7} & \textbf{12.2} & 10.2 & 12.4 & 11.1 & \textbf{11.0} & 10.5 & 12.3 & \textbf{11.4} & \textbf{13.8} & 11.0 & \textbf{10.6} & 12.7 & 12.9 & 12.4 & 11.7  \\
1F, 15 UE & Ours & 10.9 & 13.0 & \textbf{9.59} & \textbf{11.8} & \textbf{10.3} & 11.4 & \textbf{10.1} & \textbf{10.5} & 12.2 & 14.6 & \textbf{10.9} & 11.5 & \textbf{12.5} & \textbf{12.6} & \textbf{12.2} & \textbf{11.6} \\
\hline
27F, 15 UE & VideoPose3D & 3.84 & 4.02 & 3.14 & 4.42 & 3.31 & 3.58 & \textbf{3.44} & 3.93 & 3.14 & 4.21 & 3.36 & \textbf{3.34} & 4.68 & 4.51 & 4.08 & 3.80 \\
27F, 15 UE & Ours & \textbf{3.78} & \textbf{4.28} & \textbf{3.01} & \textbf{4.27} & \textbf{3.14} & \textbf{3.50} & 3.79 & \textbf{3.65} & \textbf{2.84} & \textbf{3.90} & \textbf{3.05} & 3.55 & \textbf{4.37} & \textbf{3.77} & \textbf{3.49} & \textbf{3.63}\\
\hline

243F, 1200 UE & VideoPose3D & & &  \\
243F, 1200 UE & Ours & & & 2.56\\

 
\hline

\hline
\end{tabular}

\caption{Protocol 2, MPJPE Velocity Error}
\end{table*}
\end{center}


\subsection{Large Number of Epochs}

For a large number of data, we observe that because of the characteristics of the model that \textit{VideoPose3D}
utilizes, the epoch refuses to increase after epoch 60 (i.e. epoch 4 of the original training method).


\begin{table}[H]
\caption{Property Cluster, 27F, 1200 UE}
\centering
\begin{tabular}{ccc}
\hline
Type & Original  & Eating-based\\
\hline

Time Consumed&  ? sec. & \textbf{56921} sec.\\
MPJPE-Eating&  ? mm & \textbf{49.27} mm\\
Velo-M-Eating&  ? mm & \textbf{2.56} mm  \\

\hline
\end{tabular}
\end{table}


Shown below is the relationship between our work (right) and the original work (left) using protocol MPJPE with the
lapse of epochs.
Note that each epoch worths $\frac{1}{15}$ Epoch Units.

\begin{table}[H]
\caption{Epoch-based Observations}
\centering
\begin{tabular}{cccc}
\hline
Epochs & Epoch Units & Original & Eating-based\\
\hline

1& NaN & NaN & 138.86 \\
5& NaN & NaN & 96.52  \\
10& NaN & NaN & 65.02  \\
15& 1 & ? & 60.21 \\
30& 2 & ? & 52.06 \\
45& 3 & ? & 50.28  \\
60& 4 & ? & 50.58  \\
75& 5 & ? & 50.49  \\
90& 6 & ? & 50.37 \\
450& 30 & ? & 50.53 \\
1200& 80 & ? & 50.45 \\


\hline
\end{tabular}
\end{table}

Transforming data into more readable format we gain the graph below.

\begin{figure*}
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  		\includegraphics[width=0.9\linewidth]{MPJPE_epoch_overall.png}
	\end{center}
   	\caption{Relationship between MPJPE and epoch (original)}
	\label{fig:long}
	\label{fig:onecol}
\end{figure*}



\begin{figure*}
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  		\includegraphics[width=0.9\linewidth]{MPJPE_epoch_zoomed.png}
	\end{center}
   	\caption{Relationship between MPJPE and epoch (Zoomed)}
	\label{fig:long}
	\label{fig:onecol}
\end{figure*}

\section{Conclusion}
According to the results shown above, we conclude that the action-based model has a better performance on small epochs because 
they learn faster than the original one, though it has a worse performance on the top performance.


\newpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}

